"use strict";(self.webpackChunkspiceaidocs=self.webpackChunkspiceaidocs||[]).push([[6861],{6131:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>d});var i=a(4848),o=a(8453);const s={title:"AI Gateway - Configuring LLMs",sidebar_label:"AI Gateway",description:"Learn how to configure language models and use Spice as an AI Gateway.",sidebar_position:5,pagination_prev:null,pagination_next:null},t=void 0,r={id:"features/ai-gateway/index",title:"AI Gateway - Configuring LLMs",description:"Learn how to configure language models and use Spice as an AI Gateway.",source:"@site/docs/features/ai-gateway/index.md",sourceDirName:"features/ai-gateway",slug:"/features/ai-gateway/",permalink:"/features/ai-gateway/",draft:!1,unlisted:!1,editUrl:"https://github.com/spiceai/docs/tree/trunk/spiceaidocs/docs/features/ai-gateway/index.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"AI Gateway - Configuring LLMs",sidebar_label:"AI Gateway",description:"Learn how to configure language models and use Spice as an AI Gateway.",sidebar_position:5,pagination_prev:null,pagination_next:null},sidebar:"docsSidebar"},l={},d=[{value:"Configuring Language Models",id:"configuring-language-models",level:2},{value:"Core Features",id:"core-features",level:3},{value:"Example: Configuring an OpenAI Model",id:"example-configuring-an-openai-model",level:3}];function c(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"Spice provides a high-performance, OpenAI API-compatible AI Gateway optimized for managing and scaling large language models (LLMs)."}),"\n",(0,i.jsxs)(n.p,{children:["Additionally, Spice offers tools for Enterprise Retrieval-Augmented Generation (RAG), such as SQL query across federated datasets and an advanced search feature (see ",(0,i.jsx)(n.a,{href:"/features/search",children:"Search"}),")."]}),"\n",(0,i.jsxs)(n.p,{children:["Spice also supports ",(0,i.jsx)(n.strong,{children:"full OpenTelemetry observability"}),", enabling detailed tracking of data flows and requests for full transparency and easier debugging."]}),"\n",(0,i.jsx)(n.h2,{id:"configuring-language-models",children:"Configuring Language Models"}),"\n",(0,i.jsxs)(n.p,{children:["Spice supports a variety of LLMs (see ",(0,i.jsx)(n.a,{href:"/components/models/",children:"Model Components"}),")."]}),"\n",(0,i.jsx)(n.h3,{id:"core-features",children:"Core Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Custom Tools"}),": Equip models with tools to interact with the Spice runtime."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Prompts"}),": Customize system prompts and override defaults for ",(0,i.jsx)(n.a,{href:"/api/http/chat-completions",children:(0,i.jsx)(n.code,{children:"v1/chat/completion"})}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For detailed configuration and API usage, refer to the ",(0,i.jsx)(n.a,{href:"/api",children:"API Documentation"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"example-configuring-an-openai-model",children:"Example: Configuring an OpenAI Model"}),"\n",(0,i.jsxs)(n.p,{children:["To use a language model hosted on OpenAI (or compatible), specify the ",(0,i.jsx)(n.code,{children:"openai"})," path and model ID in ",(0,i.jsx)(n.code,{children:"from"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Example ",(0,i.jsx)(n.code,{children:"spicepod.yml"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"models:\n  - from: openai:gpt-4o-mini\n    name: openai\n    params:\n      openai_api_key: ${ secrets:SPICE_OPENAI_API_KEY }\n\n  - from: openai:llama3-groq-70b-8192-tool-use-preview\n    name: groq-llama\n    params:\n      endpoint: https://api.groq.com/openai/v1\n      openai_api_key: ${ secrets:SPICE_GROQ_API_KEY }\n"})}),"\n",(0,i.jsxs)(n.p,{children:["For details, see ",(0,i.jsx)(n.a,{href:"/components/models/openai",children:"OpenAI (or Compatible) Language Models"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>t,x:()=>r});var i=a(6540);const o={},s=i.createContext(o);function t(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);