"use strict";(self.webpackChunkspiceaidocs=self.webpackChunkspiceaidocs||[]).push([[4329],{8463:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>t,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>c});var d=i(4848),s=i(8453);const l={title:"Models",sidebar_label:"Models",description:"Models YAML reference",pagination_next:null},o="Models",r={id:"reference/spicepod/models",title:"Models",description:"Models YAML reference",source:"@site/docs/reference/spicepod/models.md",sourceDirName:"reference/spicepod",slug:"/reference/spicepod/models",permalink:"/reference/spicepod/models",draft:!1,unlisted:!1,editUrl:"https://github.com/spiceai/docs/tree/trunk/spiceaidocs/docs/reference/spicepod/models.md",tags:[],version:"current",frontMatter:{title:"Models",sidebar_label:"Models",description:"Models YAML reference",pagination_next:null},sidebar:"docsSidebar",previous:{title:"Embeddings",permalink:"/reference/spicepod/embeddings"}},t={},c=[{value:"<code>models</code>",id:"models-1",level:2},{value:"<code>from</code>",id:"from",level:3},{value:"Model Source",id:"model-source",level:4},{value:"Model ID",id:"model-id",level:4},{value:"<code>name</code>",id:"name",level:3},{value:"<code>description</code>",id:"description",level:3},{value:"<code>files</code>",id:"files",level:3},{value:"<code>params</code>",id:"params",level:3},{value:"<code>datasets</code>",id:"datasets",level:3},{value:"<code>dependsOn</code>",id:"dependson",level:3}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(n.admonition,{title:"Early Preview",type:"warning",children:(0,d.jsx)(n.p,{children:"The model specifications are in early preview and are subject to change."})}),"\n",(0,d.jsx)(n.header,{children:(0,d.jsx)(n.h1,{id:"models",children:"Models"})}),"\n",(0,d.jsx)(n.p,{children:"Spice supports both traditional machine learning (ML) models and language models (LLMs). The configuration allows you to specify either type from a variety of sources. The model type is automatically determined based on the model source and files."}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"field"}),(0,d.jsx)(n.th,{children:"Description"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"name"})}),(0,d.jsx)(n.td,{children:"Unique, readable name for the model within the Spicepod."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"from"})}),(0,d.jsx)(n.td,{children:"Source-specific address to uniquely identify a model"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"description"})}),(0,d.jsx)(n.td,{children:"Additional details about the model, useful for displaying to users"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"datasets"})}),(0,d.jsx)(n.td,{children:"Datasets that the model depends on for inference"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"files"})}),(0,d.jsx)(n.td,{children:"Specify additional files, or override default files needed by the model"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:(0,d.jsx)(n.code,{children:"params"})}),(0,d.jsx)(n.td,{children:"Additional parameters to be passed to the model"})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"models-1",children:(0,d.jsx)(n.code,{children:"models"})}),"\n",(0,d.jsxs)(n.p,{children:["The ",(0,d.jsx)(n.code,{children:"models"})," section in your configuration allows you to specify one or more models to be used with your datasets."]}),"\n",(0,d.jsx)(n.p,{children:"Example:"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-yaml",children:'models:\n  - from: huggingface:huggingface.co/gpt4:latest\n    name: text_generator\n    files:\n      - path: model.safetensors\n        type: weights\n      - path: config.json\n        type: config\n      - path: tokenizer.json\n        type: tokenizer\n    params:\n      max_length: "128"\n    datasets:\n      - my_text_dataset\n'})}),"\n",(0,d.jsx)(n.h3,{id:"from",children:(0,d.jsx)(n.code,{children:"from"})}),"\n",(0,d.jsxs)(n.p,{children:["The ",(0,d.jsx)(n.code,{children:"from"})," field specifies both the source of the model (e.g Huggingface, or a local file), and the unique identifier of the model (relative to the source). The ",(0,d.jsx)(n.code,{children:"from"})," value expects the following format"]}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-yaml",children:"- from: <model_source>/<model id>\n"})}),"\n",(0,d.jsx)(n.h4,{id:"model-source",children:"Model Source"}),"\n",(0,d.jsxs)(n.p,{children:["The ",(0,d.jsx)(n.code,{children:"<model_source>"})," prefix of the ",(0,d.jsx)(n.code,{children:"from"})," field indicates where the model is sourced from:"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"huggingface:huggingface.co"})," - Models from Hugging Face"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"file:"})," - Local file paths"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"openai"})," - OpenAI (or compatible) models"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"spiceai"})," - Spice AI models"]}),"\n"]}),"\n",(0,d.jsx)(n.h4,{id:"model-id",children:"Model ID"}),"\n",(0,d.jsxs)(n.p,{children:["The ",(0,d.jsx)(n.code,{children:"<model_id>"})," suffix of the ",(0,d.jsx)(n.code,{children:"from"})," field is a unique (per source) identifier for the model:"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["For Spice AI: Supports only ML models. Represents the full path to the model in the Spice AI repository. Supports a version suffix (default to ",(0,d.jsx)(n.code,{children:"latest"}),").","\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Example: ",(0,d.jsx)(n.code,{children:"lukekim/smart/models/drive_stats:60cb80a2-d59b-45c4-9b68-0946303bdcaf"})]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["For Hugging Face: A repo_id and, optionally, revision hash or tag.","\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"Qwen/Qwen1.5-0.5B"})," (no revision)"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"meta-llama/Meta-Llama-3-8B:cd892e8f4da1043d4b01d5ea182a2e8412bf658f"})," (with revision hash)"]}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["For local files: Represents the absolute or relative path to the model weights file on the local file system. See ",(0,d.jsx)(n.a,{href:"#files",children:"below"})," for the accepted model weight types and formats."]}),"\n",(0,d.jsxs)(n.li,{children:["For OpenAI: Only supports LMs. For OpenAI models, valid IDs can be found in their model ",(0,d.jsx)(n.a,{href:"https://platform.openai.com/docs/models/continuous-model-upgrades",children:"documentation"}),". For OpenAI compatible providers, specify the value  required in their ",(0,d.jsx)(n.code,{children:"v1/chat/completion"})," ",(0,d.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/chat/create#chat-create-model",children:"payload"}),"."]}),"\n"]}),"\n",(0,d.jsx)(n.h3,{id:"name",children:(0,d.jsx)(n.code,{children:"name"})}),"\n",(0,d.jsx)(n.p,{children:"A unique identifier for this model component."}),"\n",(0,d.jsx)(n.h3,{id:"description",children:(0,d.jsx)(n.code,{children:"description"})}),"\n",(0,d.jsx)(n.p,{children:"Additional details about the model, useful for displaying to users"}),"\n",(0,d.jsx)(n.h3,{id:"files",children:(0,d.jsx)(n.code,{children:"files"})}),"\n",(0,d.jsx)(n.p,{children:"Optional. A list of files associated with this model. Each file has:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"path"}),": The path to the file"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"name"}),": Optional. A name for the file"]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"type"}),": Optional. The type of the file (automatically determined if not specified)"]}),"\n"]}),"\n",(0,d.jsx)(n.p,{children:"File types include:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.code,{children:"weights"}),": Model weights"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["For ML models: typically ",(0,d.jsx)(n.code,{children:".onnx"})," files"]}),"\n",(0,d.jsxs)(n.li,{children:["For LLMs: ",(0,d.jsx)(n.code,{children:".gguf"}),", ",(0,d.jsx)(n.code,{children:".ggml"}),", ",(0,d.jsx)(n.code,{children:".safetensors"}),", or ",(0,d.jsx)(n.code,{children:"pytorch_model.bin"})," files"]}),"\n",(0,d.jsx)(n.li,{children:"These files contain the trained parameters of the model"}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.code,{children:"config"}),": Model configuration"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Usually a ",(0,d.jsx)(n.code,{children:"config.json"})," file"]}),"\n",(0,d.jsx)(n.li,{children:"Contains model architecture and hyperparameters"}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.code,{children:"tokenizer"}),": Tokenizer file"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Usually a ",(0,d.jsx)(n.code,{children:"tokenizer.json"})," file"]}),"\n",(0,d.jsx)(n.li,{children:"Defines how input text is converted into tokens for the model"}),"\n"]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.code,{children:"tokenizer_config"}),": Tokenizer configuration"]}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["Usually a ",(0,d.jsx)(n.code,{children:"tokenizer_config.json"})," file"]}),"\n",(0,d.jsx)(n.li,{children:"Contains additional configuration for the tokenizer"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.p,{children:"The system attempts to automatically determine the file type based on the file name and extension. If the type cannot be determined automatically, you can explicitly specify it in the configuration."}),"\n",(0,d.jsx)(n.h3,{id:"params",children:(0,d.jsx)(n.code,{children:"params"})}),"\n",(0,d.jsx)(n.p,{children:"Optional. A map of key-value pairs for additional parameters specific to the model."}),"\n",(0,d.jsx)(n.h3,{id:"datasets",children:(0,d.jsx)(n.code,{children:"datasets"})}),"\n",(0,d.jsxs)(n.p,{children:["Optional. A list of ",(0,d.jsx)(n.a,{href:"/reference/spicepod/datasets#name",children:"dataset names"})," that this model should be applied to. For ML models, this preselects the dataset to use for inference."]}),"\n",(0,d.jsx)(n.h3,{id:"dependson",children:(0,d.jsx)(n.code,{children:"dependsOn"})}),"\n",(0,d.jsx)(n.p,{children:"Optional. A list of dependencies that must be loaded and available before this model."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,d.jsx)(n,{...e,children:(0,d.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var d=i(6540);const s={},l=d.createContext(s);function o(e){const n=d.useContext(l);return d.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),d.createElement(l.Provider,{value:n},e.children)}}}]);